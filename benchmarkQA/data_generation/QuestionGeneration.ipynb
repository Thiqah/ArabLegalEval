{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "762e5484",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/low_level/evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa530a77-b0a7-46f1-bb17-0cfa31650c60",
   "metadata": {},
   "source": [
    "# Building Evaluation from Scratch\n",
    "\n",
    "We show how you can build evaluation modules from scratch. This includes both evaluation of the final generated response (where the output is plain text), as well as the evaluation of retrievers (where the output is a ranked list of items).\n",
    "\n",
    "We have in-house modules in our [Evaluation](https://gpt-index.readthedocs.io/en/latest/core_modules/supporting_modules/evaluation/root.html) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69705b2b-38f4-471e-bccf-f3bac26f1582",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We load some data and define a very simple RAG query engine that we'll evaluate (uses top-k retrieval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeaaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-readers-file pymupdf\n",
    "# %pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bec520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter, SimpleNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0679d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv('../.env')\n",
    "\n",
    "# load yaml file\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "config = EasyDict(yaml.safe_load(open(\"defaults.yaml\")))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840fc355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a360d-ea79-444e-9f0f-cfcca9fb9642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "if os.path.exists('ArabicMMLU.csv'):\n",
    "    arabicmmlu_df = pd.read_csv('ArabicMMLU.csv')\n",
    "else:\n",
    "    arabicMMLU = load_dataset('MBZUAI/ArabicMMLU')\n",
    "    arabicmmlu_df = arabicMMLU['test'].to_pandas()\n",
    "    arabicmmlu_df.to_csv('ArabicMMLU.csv', index=False)\n",
    "    \n",
    "\n",
    "arabicmmlu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b422d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "\n",
    "# Set a large chunk size\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=1e6)\n",
    "\n",
    "\n",
    "\n",
    "def convert_qa_to_string(row) -> str:\n",
    "    options = list(map(str.strip, filter(lambda x: x is not None and x is not np.nan, [\n",
    "       str(row[f\"Option {i}\"]) for i in range(1, 6)\n",
    "    ])))\n",
    "    options_str = \"\\n\".join(\n",
    "        f\"{i+1}. {x}\" for i, x in enumerate(options)\n",
    "    )\n",
    "    answer_key_map = {\n",
    "        \"A\": 0,\n",
    "        \"B\": 1,\n",
    "        \"C\": 2,\n",
    "        \"D\": 3,\n",
    "        \"E\": 4,\n",
    "    }\n",
    "    correct_answer_str = options[answer_key_map[row[\"Answer Key\"]]]\n",
    "\n",
    "    return config.MCQ_PROMPT.format(\n",
    "        question=row['Question'],\n",
    "        options=options_str,\n",
    "        correct_answer=correct_answer_str\n",
    "    )\n",
    "\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        text=convert_qa_to_string(row),\n",
    "        metadata=row.to_dict()\n",
    "    ) for _, row in arabicmmlu_df.query('Subject==\"Law\"').iterrows()\n",
    "]\n",
    "\n",
    "arabicmmlu_nodes = node_parser.get_nodes_from_documents(\n",
    "    documents,\n",
    "    show_progress=True\n",
    ")\n",
    "print(arabicmmlu_nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../../data/raw/ArabLegalEval/MOJ_Regulations.json\", 'r', encoding='utf8') as f:\n",
    "    moj_regulations = json.load(f)\n",
    "\n",
    "def listify_dict(d):\n",
    "    outlist = []\n",
    "    for k, v in d.items():\n",
    "        # print('listifying', k, v)\n",
    "        if k == 'name':\n",
    "            continue\n",
    "        v['name'] = k\n",
    "        outlist.append(v)\n",
    "    return outlist\n",
    "\n",
    "\n",
    "for i in range(len(moj_regulations)):\n",
    "    first_value = next(iter(moj_regulations[i]['Subjects'].values()))\n",
    "    if 'description' in first_value:  # has no chapters\n",
    "        # print(\"No chapters in\", moj_regulations[i]['Subjects'].keys())\n",
    "        moj_regulations[i]['Subjects'] = {\n",
    "            'الباب الأول أحكام عامة': moj_regulations[i]['Subjects'],\n",
    "            # 'الباب الأول أحكام عامة': moj_regulations[i]['Subjects'],\n",
    "        }\n",
    "    # moj_regulations[i]['Subjects'] = list(moj_regulations[i]['Subjects'].values())  ##FIXME: WARNING: this drops the keys which have some metadata\n",
    "    moj_regulations[i]['Subjects'] = list(map(listify_dict, listify_dict(moj_regulations[i]['Subjects'])))\n",
    "    # drop all tables\n",
    "    for k, v in moj_regulations[i]['Subjects']:\n",
    "        if 'tables' in v:\n",
    "            del v['tables']  ##TODO: FIXME: make this append a markdown table to 'description'\n",
    "\n",
    "\n",
    "## This is the part I'm not sure of ....\n",
    "import pandas as pd\n",
    "df_regs = pd.DataFrame(moj_regulations)\n",
    "df_regs = pd.concat([\n",
    "    df_regs.drop(columns=['Details']),\n",
    "    pd.json_normalize(df_regs['Details'])\n",
    "], axis=1)\n",
    "\n",
    "df_regs = df_regs.explode('Subjects').explode('Subjects', )\n",
    "flat_df = pd.concat([\n",
    "    df_regs.drop(columns=['Subjects']).reset_index(),\n",
    "    pd.json_normalize(df_regs['Subjects'], max_level=1)\n",
    "], axis=1)\n",
    "\n",
    "flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa965444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287da53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory = '../../data/processed/Legal Data/وزارة العدل'  # Change this to your directory path\n",
    "\n",
    "keys = [\n",
    "    'preprocess_script_git_hash',\n",
    "    'untrustworthy_git_hash',\n",
    "    'schema_version',\n",
    "    'source_entity',\n",
    "    'origin_url',\n",
    "    'serial_number',\n",
    "    'original_file_path',\n",
    "    'document_type',\n",
    "    'circular_topic',\n",
    "    'circular_number',\n",
    "    'title',\n",
    "    'issue_date',\n",
    "    'effective_date',\n",
    "    'expiration_date',\n",
    "    'confidentiality',\n",
    "    'languages',\n",
    "    'contents',\n",
    "]\n",
    "# keys = ['original_file_path', 'contents', 'circular_topics']  # Change these to the keys you're interested in\n",
    "\n",
    "def clean_text_content(string):\n",
    "    return string.replace(\"\"\"## text:\\n---\"\"\", \"\").replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "\n",
    "json_paths = [file for file in glob.glob(directory + '/**/*.json', recursive=True) if os.path.isfile(file)]\n",
    "json_objects = [json.load(open(file_path, 'r')) for file_path in json_paths]\n",
    "\n",
    "df = pd.DataFrame(json_objects)\n",
    "df['paths'] = json_paths\n",
    "\n",
    "#modify the dataframe to change the contents series to only contain the text value\n",
    "df['contents'] = df['contents'].apply(lambda x: x[0]['text'] if x is not None and isinstance(x, list) and  x[0] and len(x[0]['text']) > 20 else None)\n",
    "#drop rows with null values in the content column only\n",
    "df = df.dropna(subset=['contents'])\n",
    "df['contents'] = df['contents'].apply(clean_text_content)\n",
    "\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4320016-3e22-485d-b6f7-ce017904310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contents'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = AzureOpenAI(\n",
    "    # model=\"gpt-4-32k\",\n",
    "    engine=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    api_key=os.environ['AZURE_OPENAI_API_KEY'],\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    ")\n",
    "# llm.complete('hi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaebc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cbd4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://Cohere-command-r-plus-uptoi-serverless.eastus2.inference.ai.azure.com\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    # model=\"gpt-4-32k\",\n",
    "    engine=\"Cohere-command-r-plus-uptoi\",\n",
    "    api_key=\"N3KUBd1tsfxituEDcC7GuGjdKjhpW77O\",\n",
    "    azure_endpoint=\"https://Cohere-command-r-plus-uptoi-serverless.eastus2.inference.ai.azure.com\",\n",
    "    # api_version=,\n",
    ")\n",
    "llm.complete('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6d37e-dc23-43a2-9d47-605d5978c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4868a75-0a1c-486c-81b1-bea989e62591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import PromptTemplate, ServiceContext, StorageContext, VectorStoreIndex, load_index_from_storage\n",
    "\n",
    "\n",
    "\n",
    "embed_model = OpenAIEmbedding(model='text-embedding-3-large', api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138aae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path='./chroma_db')\n",
    "# Traditional VDB\n",
    "try:\n",
    "    chroma_collection = chroma_client.get_collection(f'ArabicMMLU_legal')\n",
    "except Exception as e:\n",
    "    print(\"Creating new collection\")\n",
    "    chroma_collection = chroma_client.create_collection('ArabicMMLU_legal')\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "arabicmmlu_index = VectorStoreIndex(\n",
    "    arabicmmlu_nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    use_async=False,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# # Sentence window retrieval\n",
    "# query_engine_sentence_window = index_sentence_window.as_query_engine(\n",
    "#     text_qa_template=text_qa_template, similarity_top_k=3, embed_model=embed_model, llm=llm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "node_parser = SentenceSplitter(chunk_size=1e6, chunk_overlap=0)\n",
    "documents = [\n",
    "    Document(\n",
    "        text=row['contents'].strip(),\n",
    "        metadata=row.drop('contents').to_dict()\n",
    "    ) for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(\n",
    "    documents,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_rules = [\n",
    "    {'Rule': 'Use single best answer', '%*': 71.0},\n",
    "    # {'Rule': 'Use format vertically', '%*': 36.0},\n",
    "    # {'Rule': 'Use sound English', '%*': 64.0},\n",
    "    # {'Rule': 'Use clear simple vocabulary', '%*': 64.0},\n",
    "    # {'Rule': 'Avoid cueing or hinging', '%*': 71.0},\n",
    "    {'Rule': 'Use important significant material', '%*': 93.0},\n",
    "    {'Rule': 'Use single objective per item', '%*': 71.0},\n",
    "    # {'Rule': 'Use own novel material', '%*': 36.0},\n",
    "    # {'Rule': 'Use different thinking levels', '%*': 50.0},\n",
    "    # {'Rule': 'Avoid tricky items', '%*': 71.0},\n",
    "    # {'Rule': 'Use stem with vignette and question', '%*': 86.0},\n",
    "    {'Rule': 'Use positive stem and lead-in', '%*': 86.0},\n",
    "    # {'Rule': 'Use central idea in the stem', '%*': 86.0},\n",
    "    # {'Rule': 'Use cover-the-options rule', '%*': 50.0},\n",
    "    # {'Rule': 'Avoid absolute terms', '%*': 64.0},\n",
    "    {'Rule': 'Use plausible, homogeneous options with parallel length', '%*': 95.0},\n",
    "    # {'Rule': 'Use options in logical order without overlapping', '%*': 82.0},\n",
    "    # {'Rule': 'Avoid aOTA, NOTA or “Complex form”', '%*': 81.0},\n",
    "    {'Rule': 'Avoid vague terms', '%*': 79.0},\n",
    "    {'Rule': 'Avoid test-wise item flaws', '%*': 79.0},\n",
    "]\n",
    "\n",
    "print(\"- \" + \"\\n- \".join([x['Rule'] for x in mcq_rules]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc531f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_str = pd.read_csv('mcq_rules.csv').dropna().to_dict()\n",
    "apply(\n",
    "    # for each row, join rule and item format\n",
    "    lambda row: row['Rule'] + \": \" + row[\"Item format\"],\n",
    "    axis=1\n",
    ")\n",
    "print(\"\\n\".join(list(rules_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f7975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86bec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60e745-aae4-4ab2-be7d-563a883d27a3",
   "metadata": {},
   "source": [
    "## Dataset Generation\n",
    "\n",
    "We first go through an exercise of generating a synthetic evaluation dataset. We do this by synthetically generating a set of questions from existing context. We then run each question with existing context through a powerful LLM (e.g. GPT-4) to generate a \"ground-truth\" response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4f0bf-8f7d-4693-b055-d050020b1291",
   "metadata": {},
   "source": [
    "### Define Functions\n",
    "\n",
    "We define the functions that we will use for dataset generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a305907-14d1-4f5c-a15c-145ae7dcb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.schema import BaseNode\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate, PromptTemplate\n",
    "from typing import Tuple, List\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing.pool import ThreadPool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70019ed9-bed5-434e-b6d3-545726ac7397",
   "metadata": {},
   "source": [
    "We define `generate_answers_for_questions` to generate answers from questions given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca84683-5d4c-4b1f-ae4d-cd6aea51dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.USER, content=config.QA_PROMPT),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def generate_answers_for_questions(\n",
    "    questions: List[str], context: str, llm: OpenAI\n",
    ") -> str:\n",
    "    \"\"\"Generate answers for questions given context.\"\"\"\n",
    "    \n",
    "    def generate_answer(idx, question):\n",
    "        fmt_qa_prompt = question_answer_template.format_messages(\n",
    "            context_str=context,\n",
    "            query_str=question,\n",
    "        )\n",
    "        response_obj = llm.chat(fmt_qa_prompt)\n",
    "        return response_obj.message.content\n",
    "\n",
    "    # for idx, node in enumerate(nodes):\n",
    "    answers = list(\n",
    "        tqdm(\n",
    "            ThreadPool().imap(\n",
    "                lambda x: generate_answer(*x),\n",
    "                enumerate(questions),\n",
    "            ),\n",
    "        \"generate_answers_for_questions()\",\n",
    "        total=len(questions),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0e168-e2d4-4e63-ae77-5413499b0c7d",
   "metadata": {},
   "source": [
    "We define `generate_qa_pairs` to generate qa pairs over an entire list of Nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4e96d-d08d-4c69-9b35-0dbb1f0a61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION_GEN_USER_TMPL = (\n",
    "#     \"Context information is below.\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"Given the context information and not prior knowledge, \"\n",
    "#     \"generate the relevant questions. \"\n",
    "# )\n",
    "\n",
    "# QUESTION_GEN_SYS_TMPL = \"\"\"\\\n",
    "# You are a Teacher/ Professor. Your task is to setup \\\n",
    "# {num_questions_per_chunk} questions for an upcoming \\\n",
    "# quiz/examination. The questions should be diverse in nature \\\n",
    "# across the document. Restrict the questions to the \\\n",
    "# context information provided.\\\n",
    "# \"\"\"\n",
    "\n",
    "question_gen_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=config.QUESTION_GEN_SYS_TMPL),\n",
    "        ChatMessage(role=MessageRole.USER, content=config.QUESTION_GEN_USER_TMPL),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def generate_qa_pairs(\n",
    "    nodes: List[BaseNode], llm: OpenAI, num_questions_per_chunk: int = 2,\n",
    "    delimiter: str = \"\\n\",\n",
    "    question_gen_template=question_gen_template,\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"Generate questions.\"\"\"\n",
    "    #TODO: add support for few shot prompts (using index retriever)\n",
    "    def process_node(idx, node):\n",
    "        context_str = node.get_content(metadata_mode=\"all\")\n",
    "        fmt_messages = question_gen_template.format_messages(\n",
    "            num_questions_per_chunk=num_questions_per_chunk,\n",
    "            context_str=context_str,\n",
    "        )\n",
    "        chat_response = llm.chat(fmt_messages)\n",
    "        raw_output = chat_response.message.content\n",
    "\n",
    "        result_list = str(raw_output).strip().split(delimiter)\n",
    "        cleaned_questions = [\n",
    "            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip()\n",
    "            for question in result_list\n",
    "        ]\n",
    "        answers = generate_answers_for_questions(\n",
    "            cleaned_questions, context_str, llm\n",
    "        )\n",
    "        cur_qa_pairs = list(zip(cleaned_questions, answers))\n",
    "        return cur_qa_pairs\n",
    "    \n",
    "    qa_pairs = list(\n",
    "        tqdm(\n",
    "            ThreadPool().imap(\n",
    "                lambda x: process_node(*x),\n",
    "                enumerate(nodes),\n",
    "            ),\n",
    "        \"Generating QA pairs\",\n",
    "        total=len(nodes),\n",
    "        )\n",
    "    )\n",
    "    # flatten\n",
    "    qa_pairs = [item for sublist in qa_pairs for item in sublist]\n",
    "        \n",
    "    return qa_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arabicmmlu_df = arabicmmlu_df.drop('ID', axis=1)\n",
    "# arabicmmlu_df['text'] = arabicmmlu_df['contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b654eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "(nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 random indicies to select nodes\n",
    "import random\n",
    "random_indicies = random.sample(range(0, len(nodes)), 10)\n",
    "# random_indicies = [1138, 2673, 883, 745, 2670, 2801, 1748, 2999, 632, 72]\n",
    "random_nodes = [nodes[i] for i in random_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30455ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = generate_qa_pairs(\n",
    "    random_nodes,\n",
    "    # nodes,\n",
    "    llm,\n",
    "    num_questions_per_chunk=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc86bed-c5f9-4bc4-939f-49bca7599fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q, a in qa_pairs:\n",
    "    print(f\"Q: {q}\\nA: {a}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d98305",
   "metadata": {},
   "source": [
    "Converting question answer paris int MSQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_to_mcq_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.USER, content=config.QA_TO_MCQ_PROMPT),\n",
    "    ]\n",
    ")\n",
    "qa_to_mcq_cot_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.USER, content=config.QA_TO_MCQ_COT_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb47b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def convert_quesitons_to_mcqs(\n",
    "    qa_pairs: List[tuple], mcq_prompt_template: str, llm: OpenAI\n",
    ") -> str:\n",
    "    \"\"\"Converting question-answer paris into MCQs.\"\"\"\n",
    "    \n",
    "    def question_to_mcq(idx, qa_pair):\n",
    "        question, answer = qa_pair\n",
    "        prompt_template = mcq_prompt_template.format_messages(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "        )\n",
    "        response_obj = llm.chat(prompt_template)\n",
    "        return response_obj.message.content\n",
    "\n",
    "    mcqs = list(\n",
    "        tqdm(\n",
    "            ThreadPool().imap(\n",
    "                lambda x: question_to_mcq(*x),\n",
    "                enumerate(qa_pairs),\n",
    "            ),\n",
    "        \"convert_quesitons_to_mcqs()\",\n",
    "        total=len(qa_pairs),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return mcqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqs = convert_quesitons_to_mcqs(\n",
    "    qa_pairs,\n",
    "    qa_to_mcq_template,\n",
    "    llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b541f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mcqs(mcqs):\n",
    "    formatted_mcqs = []\n",
    "    for example in mcqs:\n",
    "        question = example.split('\\n')[0]\n",
    "        options = example.split('\\n')[2:]\n",
    "        formatted_mcqs.append([question, options])\n",
    "    return formatted_mcqs\n",
    "\n",
    "formated_mcqs = format_mcqs(mcqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mcq_qa_pairs.json\", \"w\") as f:\n",
    "    json.dump(formated_mcqs, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64bba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mcq in mcqs:\n",
    "    print(mcq)\n",
    "    print('--------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9c11e0",
   "metadata": {},
   "source": [
    "With Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqs_cot = convert_quesitons_to_mcqs(\n",
    "    qa_pairs,\n",
    "    qa_to_mcq_cot_template,\n",
    "    llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfdd01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mcq in mcqs_cot:\n",
    "    print(mcq)\n",
    "    print('--------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a40849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mcqs_cot(mcqs_cot):\n",
    "    formatted_mcqs_cot = []\n",
    "    for example in mcqs_cot:\n",
    "        reasoning_split = example.strip().strip('####').split('####')\n",
    "        reasoning = reasoning_split[0]\n",
    "        question = reasoning_split[1].strip().split('\\n')[0]\n",
    "        options = reasoning_split[1].strip().split('\\n')[2:]\n",
    "        formatted_mcqs_cot.append([question,reasoning, options])\n",
    "    return formatted_mcqs_cot\n",
    "\n",
    "formated_mcqs_cot = format_mcqs_cot(mcqs_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7abd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mcq_qa_cot_pairs.json\", \"w\") as f:\n",
    "    json.dump(formated_mcqs_cot, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54f0b3-ba1b-40ca-bad3-b3e7f52ff832",
   "metadata": {},
   "source": [
    "### Getting Pairs over Dataset\n",
    "\n",
    "**NOTE**: This can take a long time. For the sake of speed try inputting a subset of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2bd4c-059f-4c6a-b151-e591eb5d16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_pairs = generate_qa_pairs(\n",
    "#     # nodes[:1],\n",
    "#     nodes,\n",
    "#     llm,\n",
    "#     question_gen_template=question_gen_template,\n",
    "#     num_questions_per_chunk=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3abef",
   "metadata": {},
   "source": [
    "#### For MCQ GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37459d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mcq_question_gen_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=config.MCQ_QUESTION_GEN_SYS_TMPL),\n",
    "        ChatMessage(role=MessageRole.USER, content=config.MCQ_QUESTION_GEN_USER_TMPL),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def generate_mcq_pairs(\n",
    "    nodes: List[BaseNode], llm: OpenAI, num_questions_per_chunk: int = 10,\n",
    "    top_k: int = 3,\n",
    "    delimiter: str = \"\\n\",\n",
    "    mcq_question_gen_template=mcq_question_gen_template,\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"Generate questions.\"\"\"\n",
    "    def process_node(idx, node):\n",
    "        context_str = node.get_content(metadata_mode=\"none\")\n",
    "        if \"{few_shot_examples}\" in '\\n'.join([x.content for x in mcq_question_gen_template.message_templates]) and top_k > 0:\n",
    "            arabicmmlu_retriever = arabicmmlu_index.as_retriever(\n",
    "                similarity_top_k=top_k,\n",
    "                embed_model=embed_model,\n",
    "            )\n",
    "            few_shot_examples_str = \"\\n------------------------------------------------\\n\".join([\n",
    "                x.text for x in arabicmmlu_retriever.retrieve(node.text)\n",
    "            ])\n",
    "        else:\n",
    "            few_shot_examples_str = \"\"\n",
    "\n",
    "        fmt_messages = mcq_question_gen_template.format_messages(\n",
    "            num_questions_per_chunk=num_questions_per_chunk,\n",
    "            context_str=context_str,\n",
    "            few_shot_examples=few_shot_examples_str,\n",
    "        )\n",
    "        chat_response = llm.chat(fmt_messages)\n",
    "        raw_output = chat_response.message.content\n",
    "        result_list = str(raw_output).strip().split(delimiter)\n",
    "\n",
    "        cur_mcq_pairs = [\n",
    "            #TODO: make this from the config\n",
    "            question.split(\"الجواب الصحيح: \")\n",
    "            for question in result_list if question.strip()\n",
    "        ]\n",
    "\n",
    "        return cur_mcq_pairs\n",
    "    \n",
    "    mcq_pairs = list(\n",
    "        tqdm(\n",
    "            ThreadPool().imap(\n",
    "                lambda x: process_node(*x),\n",
    "                enumerate(nodes),\n",
    "            ),\n",
    "        \"Generating QA pairs\",\n",
    "        total=len(nodes),\n",
    "        )\n",
    "    )\n",
    "    # flatten\n",
    "    mcq_pairs = [item for sublist in mcq_pairs for item in sublist]\n",
    "        \n",
    "    return mcq_pairs\n",
    "\n",
    "mcq_pairs = generate_mcq_pairs(\n",
    "    random_nodes,\n",
    "    # nodes,\n",
    "    llm,\n",
    "    mcq_question_gen_template=mcq_question_gen_template,\n",
    "    num_questions_per_chunk=4,\n",
    "    delimiter=\"####\"\n",
    ")\n",
    "\n",
    "# Run your async function in the existing event loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q, a in mcq_pairs:\n",
    "    print(f\"Q: {q}\\nA: {a}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aba76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [0, 1, 3, 5]:\n",
    "    mcq_pairs = generate_mcq_pairs(\n",
    "        random_nodes,\n",
    "        llm,\n",
    "        mcq_question_gen_template=mcq_question_gen_template,\n",
    "        num_questions_per_chunk=1,\n",
    "        top_k=k,\n",
    "        delimiter=\"####\"\n",
    "    )\n",
    "    with open(f\"mcq_pairs_top_{k}.json\", \"w\") as f:\n",
    "        json.dump(mcq_pairs, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def w(*args, **kwargs):\n",
    "    return print(textwrap.fill(args[0], width=80), *args[1:], **kwargs)\n",
    "\n",
    "# print(fmt_messages[-1].content)\n",
    "# print wrapped\n",
    "\n",
    "# print(textwrap.fill(fmt_messages[-1].content, width=80))\n",
    "# w(fmt_messages[-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca7789-1675-4df3-8935-4e2d4e49389c",
   "metadata": {},
   "source": [
    "#### [Optional] Define save/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68faa352-a281-440a-aee3-e74e214d2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "import pickle\n",
    "\n",
    "pickle.dump(qa_pairs, open(\"eval_dataset.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99e1b6-cf45-4e2e-bbd1-5ee3b6b1e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "import pickle\n",
    "\n",
    "qa_pairs = pickle.load(open(\"eval_dataset.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee53e5f-005b-4795-b847-9b6440785527",
   "metadata": {},
   "source": [
    "## Evaluating Generation\n",
    "\n",
    "In this section we walk through a few methods for evaluating the generated results. At a high-level we use an \"evaluation LLM\" to measure the quality of the generated results. We do this in both the **with labels** setting and **without labels** setting. \n",
    "\n",
    "We go through the following evaluation algorithms:\n",
    "- **Correctness**: Compares the generated answer against the ground-truth answer.\n",
    "- **Faithfulness**: Evaluates whether a response is faithful to the contexts (label-free)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a7ac0-32f2-4a86-b1a7-f726334ec749",
   "metadata": {},
   "source": [
    "### Building a Correctness Evaluator\n",
    "\n",
    "The correctness evaluator compares the generated answer to the reference ground-truth answer, given the query. We output a score between 1 and 5, where 1 is the worst and 5 is the best.\n",
    "\n",
    "We do this through a system and user prompt with a chat interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e2cff-459b-412f-9282-1976e5c60db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate, PromptTemplate\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4f4e7-4512-4d46-b90a-6e3a075da7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdf76f-3a36-4dbf-9699-4007453eeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chat_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=config.QA_CORRECTNESS_SYS_TMPL),\n",
    "        ChatMessage(role=MessageRole.USER, content=config.QA_CORRECTNESS_USER_TMPL),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6f277-760a-404f-9d7f-0b89138bd0f4",
   "metadata": {},
   "source": [
    "Now that we've defined the prompts template, let's define an evaluation function that feeds the prompt to the LLM and parses the output into a dict of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cde5d8-2b33-45b3-b457-17bd73bd020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "def run_correctness_eval(\n",
    "    query_str: str,\n",
    "    reference_answer: str,\n",
    "    generated_answer: str,\n",
    "    llm: OpenAI,\n",
    "    threshold: float = 4.0,\n",
    ") -> Dict:\n",
    "    \"\"\"Run correctness eval.\"\"\"\n",
    "    fmt_messages = eval_chat_template.format_messages(\n",
    "        llm=llm,\n",
    "        query=query_str,\n",
    "        reference_answer=reference_answer,\n",
    "        generated_answer=generated_answer,\n",
    "    )\n",
    "    chat_response = llm.chat(fmt_messages)\n",
    "    raw_output = chat_response.message.content\n",
    "\n",
    "    # Extract from response\n",
    "    score_str, reasoning_str = raw_output.split(\"\\n\", 1)\n",
    "    score = float(score_str)\n",
    "    reasoning = reasoning_str.lstrip(\"\\n\")\n",
    "\n",
    "    return {\"passing\": score >= threshold, \"score\": score, \"reason\": reasoning}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f467d-85bf-403e-b2ba-eb76504a5483",
   "metadata": {},
   "source": [
    "Now let's try running this on some sample inputs with a chat model (GPT-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc8e11c-920d-454d-8905-3b5018425a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109fb2f3-ee93-4292-8c1c-da4d0584673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "index = VectorStoreIndex(nodes, show_progress=True)\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85d69f-e223-4730-91e2-8d885ff1cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_str = \"What is the range of parameters for the large language models (LLMs) developed in this work?\"\n",
    "# reference_answer = \"The range of parameters for the large language models (LLMs) developed in this work is from 7 billion to 70 billion.\"\n",
    "\n",
    "query_str = (\n",
    "    \"What is the specific name given to the fine-tuned LLMs optimized for\"\n",
    "    \" dialogue use cases?\"\n",
    ")\n",
    "reference_answer = (\n",
    "    \"The specific name given to the fine-tuned LLMs optimized for dialogue use\"\n",
    "    \" cases is Llama 2-Chat.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0197b-5cb6-4aef-a5b4-311c45d06b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answer = str(query_engine.query(query_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3721e6-b7b0-4e09-ac06-bacf8cde1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(generated_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe9ca4-9bce-4e57-bc5e-395cb119778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = run_correctness_eval(\n",
    "    query_str, reference_answer, generated_answer, llm=llm, threshold=4.0\n",
    ")\n",
    "display(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2817a49-b90a-4ca8-a28e-2f1c9965660e",
   "metadata": {},
   "source": [
    "### Building a Faithfulness Evaluator\n",
    "\n",
    "The faithfulness evaluator evaluates whether the response is faithful to any of the retrieved contexts.\n",
    "\n",
    "This is a step up in complexity from the correctness evaluator. Since the set of contexts can be quite long, they might overflow the context window. We would need to figure out how to implement a form of **response synthesis** strategy to iterate over contexts in sequence.\n",
    "\n",
    "We have a corresponding tutorial showing you [how to build response synthesis from scratch](https://gpt-index.readthedocs.io/en/latest/examples/low_level/response_synthesis.html). We also have [out-of-the-box response synthesis modules](https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/response_synthesizers/root.html). In this guide we'll use the out of the box modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c822cdc-d483-43cb-a34d-fc2d1ba7e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL_TEMPLATE = PromptTemplate(config.EVAL_TEMPLATE)\n",
    "\n",
    "# EVAL_REFINE_TEMPLATE = PromptTemplate(config.EVAL_REFINE_TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca2f5d-a063-42f4-bf32-6017d2f84830",
   "metadata": {},
   "source": [
    "**NOTE**: In the current response synthesizer setup we don't separate out a system and user message for chat endpoints, so we just use our standard `llm.complete` for text completion.\n",
    "\n",
    "We now define our function below. Since we defined both a standard eval template for a given piece of context but also a refine template for subsequent contexts, we implement our \"create-and-refine\" response synthesis strategy to obtain the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da089cb4-7bfb-4d44-b10e-2c9d52918815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import Refine\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def run_faithfulness_eval(\n",
    "    generated_answer: str,\n",
    "    contexts: List[str],\n",
    "    llm: OpenAI,\n",
    ") -> Dict:\n",
    "    \"\"\"Run faithfulness eval.\"\"\"\n",
    "\n",
    "    refine = Refine(\n",
    "        llm=llm,\n",
    "        text_qa_template=PromptTemplate(config.EVAL_TEMPLATE),\n",
    "        refine_template=PromptTemplate(config.EVAL_REFINE_TEMPLATE),\n",
    "    )\n",
    "\n",
    "    response_obj = refine.get_response(generated_answer, contexts)\n",
    "    response_txt = str(response_obj)\n",
    "\n",
    "    if \"yes\" in response_txt.lower():\n",
    "        passing = True\n",
    "    else:\n",
    "        passing = False\n",
    "\n",
    "    return {\"passing\": passing, \"reason\": str(response_txt)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570aea1-627a-44cd-8b87-251ea4037b88",
   "metadata": {},
   "source": [
    "Let's try it out on some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d6f20-9977-4bfe-b77f-fc62ba86c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same query_str, and reference_answer as above\n",
    "# query_str = \"What is the specific name given to the fine-tuned LLMs optimized for dialogue use cases?\"\n",
    "# reference_answer = \"The specific name given to the fine-tuned LLMs optimized for dialogue use cases is Llama 2-Chat.\"\n",
    "\n",
    "response = query_engine.query(query_str)\n",
    "generated_answer = str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01348f-f511-4866-8d6a-899b93bc848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = [n.get_content() for n in response.source_nodes]\n",
    "eval_results = run_faithfulness_eval(\n",
    "    generated_answer,\n",
    "    contexts=context_list,\n",
    "    llm=llm,\n",
    ")\n",
    "display(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951ef84-6a5f-4627-9a82-525d0f4bdbac",
   "metadata": {},
   "source": [
    "## Running Evaluation over our Eval Dataset\n",
    "\n",
    "Now let's tie the two above sections together and run our eval modules over our eval dataset!\n",
    "\n",
    "**NOTE**: For the sake of speed/cost we extract a very limited sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f51d6c-968b-4393-a4be-963f6b4bb51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample_size = 5\n",
    "qa_pairs_sample = random.sample(qa_pairs, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19661f-c2d1-4952-a145-051c965efb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def run_evals(qa_pairs: List[Tuple[str, str]], llm: OpenAI, query_engine):\n",
    "    results_list = []\n",
    "    for question, reference_answer in qa_pairs:\n",
    "        response = query_engine.query(question)\n",
    "        generated_answer = str(response)\n",
    "        correctness_results = run_correctness_eval(\n",
    "            query_str,\n",
    "            reference_answer,\n",
    "            generated_answer,\n",
    "            llm=llm,\n",
    "            threshold=4.0,\n",
    "        )\n",
    "        faithfulness_results = run_faithfulness_eval(\n",
    "            generated_answer,\n",
    "            contexts=context_list,\n",
    "            llm=llm,\n",
    "        )\n",
    "        cur_result_dict = {\n",
    "            \"correctness\": correctness_results[\"passing\"],\n",
    "            \"faithfulness\": faithfulness_results[\"passing\"],\n",
    "        }\n",
    "        results_list.append(cur_result_dict)\n",
    "    return pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db7677-c6db-409a-a9e2-bc38a9395da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_df = run_evals(qa_pairs_sample, llm, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af97796-1472-4246-ab9a-ecd6640326b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_df[\"correctness\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b19c8-e234-42d0-bfd1-290124ce6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_df[\"faithfulness\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thiqatiBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
