{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from openai import OpenAI\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_mcqs = 'file_path_here'\n",
    "with open(path_to_mcqs, 'r') as f:\n",
    "    mcqs = json.load(f)\n",
    "    \n",
    "len(mcqs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqs_df = pd.DataFrame(columns=['Engine', 'Context', 'Question', 'Correct Answer', 'Option 1', 'Option 2', 'Option 3', 'Option 4', 'Answer Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_question_to_df(question, mcqs_df):\n",
    "    question_dict = {}\n",
    "    for line in question.split('\\n'):\n",
    "        if line.startswith('Engine:'):\n",
    "            question_dict['Engine'] = line.split('Engine:')[1].strip()\n",
    "        elif line.startswith('Context:'):\n",
    "            question_dict['Context'] = line.split('Context:')[1].strip()\n",
    "        elif line.startswith('السؤال'):\n",
    "            question_dict['Question'] = line.split(':')[1].strip()\n",
    "        elif line.startswith('الإجابة الصحيحة:'):\n",
    "            question_dict['Correct Answer'] = line.split('الإجابة الصحيحة:')[1].strip()\n",
    "        elif line.startswith('1.'):\n",
    "            question_dict['Option 1'] = line.split('1.')[1].strip()\n",
    "        elif line.startswith('2.'):\n",
    "            question_dict['Option 2'] = line.split('2.')[1].strip()\n",
    "        elif line.startswith('3.'):\n",
    "            question_dict['Option 3'] = line.split('3.')[1].strip()\n",
    "        elif line.startswith('4.'):\n",
    "            question_dict['Option 4'] = line.split('4.')[1].strip()\n",
    "    question_dict['Answer Key'] = '1' # Default answer key is Option 1\n",
    "    mcqs_df = pd.concat([mcqs_df, pd.DataFrame([question_dict])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in mcqs:\n",
    "    append_question_to_df(question, mcqs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqs_df.to_csv('MCQs.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_question(row):\n",
    "    question = f\"سياق النص: {row['Context']}\\n\\nالسؤال: {row['Question']}\\n\\n1.{row['Option 1']}\\n2.{row['Option 2']}\\n3.{row['Option 3']}\\n4.{row['Option 4']}\\n\\nالإجابة الصحيحة: {row['Correct Answer']}\"\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a law Professor.\n",
      "You have a bank of multiple choice questions.\n",
      "Your task is to filter out the questions that are not relevant to the context information provided.\n",
      "The questions are in Arabic.\n",
      "For the following context and question:\n",
      "Question: {question}\n",
      "\n",
      "Answer the following:\n",
      "- Is the question complete, meaning it has a context, question, correct answer, and distractors? 0 for NO, 1 for YES\n",
      "- Is the question relevant to the context information? 0 for NO, 1 for YES\n",
      "- Are the ditractors for the question all incorrect? 0 for NO, 1 for YES\n",
      "- Is the correct answer the first option? 0 for NO, 1 for YES\n",
      "- Does the question need the provided context to be answered? 0 for NO, 1 for YES\n",
      "- Are all the distractors unique? 0 for NO, 1 for YES\n",
      "\n",
      "Provide the answers in the following format, Do not output anything else:\n",
      "complete_mcq: 0 or 1\n",
      "question_relevance: 0 or 1\n",
      "distractors_correctness: 0 or 1\n",
      "correct_answer_first: 0 or 1\n",
      "context_needed: 0 or 1\n",
      "unique_distractors: 0 or 1\n",
      "total_score: between 0 and 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = EasyDict(yaml.safe_load(open(\"defaults.yaml\")))\n",
    "print(config.FILTER_MCQ_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCQEvaluator:\n",
    "    def __init__(self, evaluation_prompt):\n",
    "        \"\"\"\n",
    "        A GPT model that translates text from Arabic to English and vice versa.\n",
    "        \"\"\"\n",
    "        self.client = OpenAI()\n",
    "        self.evaluation_prompt = evaluation_prompt         \n",
    "    \n",
    "    def Evaluate(self, mcq):\n",
    "        \n",
    "        completion = self.client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": self.evaluation_prompt.format(question=mcq)},\n",
    "\n",
    "        ]\n",
    "        )\n",
    "        \n",
    "        eval = completion.choices[0].message.content\n",
    "        \n",
    "        return eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MCQEvaluator(config.FILTER_MCQ_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_evaluation(evaluation):\n",
    "    evaluation = evaluation.split(\"\\n\")\n",
    "    evaluation_dict = {}\n",
    "    for line in evaluation:\n",
    "        key, value = line.split(\": \")\n",
    "        evaluation_dict[key] = int(value)\n",
    "    assert len(evaluation_dict) == 7\n",
    "    return evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval(index, row):\n",
    "    question = reconstruct_question(row)\n",
    "    evaluation = evaluator.Evaluate(question)\n",
    "    try: \n",
    "        evaluation_dict = parse_evaluation(evaluation)\n",
    "    except Exception as e:\n",
    "        print(evaluation)\n",
    "        print(f\"Error evaluating question {index}: {e}\")\n",
    "        evaluation_dict = {'contains_mcq': 0, 'question_relevance': 0, 'distractors_correctness': 0, 'correct_answer_first': 0, 'context_needed': 0, 'unique_distractors': 0, 'total_score': 0}\n",
    "    return evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqs_df = pd.read_csv('MCQs.csv')\n",
    "mcqs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "for i in tqdm(range(0, len(gpt_4_df), batch_size), f\"Evaluating New Prompt MCQs\"):\n",
    "    evaluations.extend(list(\n",
    "        ThreadPool().imap(\n",
    "            lambda x: get_eval(*x),\n",
    "            mcqs_df.iloc[i:i+batch_size].iterrows()\n",
    "        )))\n",
    "    print(evaluations[-1])\n",
    "    with open('mcqs_newprompt_eval.json', 'w') as f:\n",
    "        json.dump(evaluations, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(columns=['complete_mcq', 'question_relevance', 'distractors_correctness', 'correct_answer_first', 'context_needed', 'unique_distractors', 'total_score'])\n",
    "\n",
    "for evaluation in evaluations:\n",
    "    eval_df = pd.concat([eval_df, pd.DataFrame([evaluation])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_df = pd.concat([mcqs_df, eval_df], axis=1)\n",
    "append_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_df.to_csv('MCQs_evaluated.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = append_df[append_df['total_score'] == 6].dropna()\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def shuffle_options(row):\n",
    "    # Extract the options into a list\n",
    "    correct_answer = row['Option 1']\n",
    "    options = [row['Option 1'], row['Option 2'], row['Option 3'], row['Option 4']]\n",
    "    \n",
    "    # Shuffle the options\n",
    "    np.random.shuffle(options)\n",
    "    \n",
    "    # Find the new index of the correct answer\n",
    "    new_answer_key = options.index(correct_answer) \n",
    "    \n",
    "    # Update the row with the shuffled options and new answer key\n",
    "    row['Option 1'], row['Option 2'], row['Option 3'], row['Option 4'] = options\n",
    "    row['Answer Key'] = new_answer_key\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "shuffled_filtered_df = filtered_df.apply(shuffle_options, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-q-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
